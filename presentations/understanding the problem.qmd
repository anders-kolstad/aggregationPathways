---
title: "Understanding the problem of when to scale"
format: 
  revealjs:
    smaller: true
    scrollable: false
    preview-links: true
    embed-resources: true
editor: visual
---

```{r setup}
library(knitr)
```

## This project

-   250 000 kr

-   166 hours

-   1 month

-   Manuscript on pre-print server

-   Produce guidelines

## Blåbærkrisa

::: columns
::: {.column width="50%"}
-   Same data gave different indicator values.
-   Reference values are at county scales (and probably only make sense at that scale)
-   IBECA normalised the variables (with truncation) at plot scale
-   NI normalised the variable (probably) at a regional scale after first taking a mean
:::

::: {.column width="50%"}
![](img/billberry.PNG){width="80%"}
:::
:::

------------------------------------------------------------------------

![](img/billberry_graph.PNG)

------------------------------------------------------------------------

![](img/billberry_ibeca.PNG)

## It matters

if we aggregate (takes means of) of the scaled or unscaled variabel:

![](img/ASPT.png)

## Pathways

```{r}
source("../R/pathways.R")
pathways
```

## Glossery

**Scaling** = Linear range scaling, usually reducing the range of the variable by using one or two reference value

**Truncating** = Cutting of value above or below a given threshold. In our case, making the metrices bound between 0 and 1. Truncation is also a type of scaling.

**Re-scaling** = not used, but often used as a synonym to normalisation.

**Normalising** = a combination of scaling, followed by truncating (if needed). This lead to a non-linear transformation of the original variable and returns indicators that share the same scale.

**Non-linear scaling**: Scaling functions like truncation, sigmoid, exponential or break-point types.




## Spatially aggregating

condition estimates (bottom) or actual measurements (top)

```{r}
source("../R/example1.R")
example1
```

::: aside
Indicators with grey background
:::

------------------------------------------------------------------------
## Commutativty

![](img/commutativity.PNG)

## Early scaling

leads to cummutativity

```{r}
#| fig-align: "left"
#| fig-height: 6
#| fig-width: 7
source("../R/example3.R")
example3
```

------------------------------------------------------------------------

## Early scaling

leads to cummutativity

```{r}
#| fig-align: "left"
#| fig-height: 6
#| fig-width: 7
source("../R/example4.R")
example4
```

------------------------------------------------------------------------

## Also with area weighting

```{r}
#| fig-align: "left"
#| fig-height: 6
#| fig-width: 7
source("../R/example3b.R")
example3b
```

------------------------------------------------------------------------

## Also with area weighting

```{r}
#| fig-align: "left"
#| fig-height: 6
#| fig-width: 7
source("../R/example5.R")
example5
```


## Alternativey,

we can aggregate the actual measurements. When would this make sense to do?

```{r}
#| fig-align: "left"
#| fig-height: 6
#| fig-width: 7
source("../R/example6.R")
example6
```

------------------------------------------------------------------------

## Same, but with no area weighting and taking the sum (e.g. population sizes)

```{r}
#| fig-align: "left"
#| fig-height: 6
#| fig-width: 7
source("../R/example7.R")
example7
```


## Why do we normalise

*notes from Bård*

Reference values serve two purposes

-   to enable rescaling to a common measurement scale to facilitate the calculation of the mean.
-   to set a limit for how much one indicator can compensate for other indicators being in a bad state.

As a consequence, an index of rescaled indicators summarizes **negative deviations** from the reference state over a large set of indicators.

Unscaled states above the reference value are not recognized as being better than the reference state.

Rescaling is useful for making indeces, but why use it for individual indicators? \[answer: it depends if we want to aggregate 'condition estimates' of actuall measurements\]


## Arguments for scaling and truncating immediately

-   High or low *outliers* can not compensate for low or high values (i.e. condition) elsewhere, respectively.
-   Gives higher-resolution indicator values and preserves the original spatial resolution of the variable.
-   Means that subsequent aggregation is carrying information about the actual condition, and not the raw variable.

## Arguments for scaling and truncating later at a level where the reference value is sensible

-   The reference value was maybe intended for that scale
-   The indicator uncertainty becomes (apparently) smaller because the mean or sum for a region is less variable.
-   Maybe, too many truncation events leads to accumulated displacement errors?

## What if

-   The reference value is at the same scale as the variable
-   The reference value is the same all over
-   We also want to aggregate the original variable further, e.g. to national level - will the indicator and variable remain correlated
-   We want to use non-linear re-scaling functions with asymptotes  

## Resources

[Sanvik 2019](https://brage.nina.no/nina-xmlui/handle/11250/2631056)

